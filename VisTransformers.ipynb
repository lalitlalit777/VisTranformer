{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "from torchsummary import summary\n",
        "import math\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "r-zARKM3Xe8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7_V-Dbjrw_a"
      },
      "outputs": [],
      "source": [
        "class LightViT(nn.Module):\n",
        "  def __init__(self,image_dim, n_patches=7, n_blocks=2, d=8, n_heads=2, num_classes=10,pixel_patch=4):\n",
        "    super(LightViT, self).__init__()\n",
        "\n",
        "    self.image_dim = image_dim\n",
        "    self.n_patches = n_patches\n",
        "    self.n_blocks = n_blocks\n",
        "    self.d = d\n",
        "    self.n_heads = n_heads\n",
        "    self.num_classes = num_classes\n",
        "    self.pixel_patch = pixel_patch\n",
        "\n",
        "\n",
        "    ## Class Members\n",
        "\n",
        "    ## 1B) Linear Mapping\n",
        "    self.linear_map = linearmap(self.image_dim,self.n_patches,self.d);\n",
        "\n",
        "\n",
        "    ## 2A) Learnable Parameter\n",
        "    self.special_token = nn.Parameter(torch.randn(1,1, self.d));\n",
        "\n",
        "\n",
        "    ## 2B) Positional embedding\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ## 3) Encoder blocks\n",
        "\n",
        "    self.encoder = ViTEncoder(self.d, self.n_heads)\n",
        "\n",
        "    # 5) Classification Head\n",
        "    self.classifier = nn.Linear(self.d,self.num_classes);\n",
        "\n",
        "\n",
        "  def forward(self, images):\n",
        "    ## Extract patches\n",
        "    patch =  create_patches(images,self.n_patches)\n",
        "\n",
        "\n",
        "\n",
        "    ## Linear mapping\n",
        "\n",
        "    linear_output = self.linear_map(patch)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ## Add classification token\n",
        "    b, _, _, _ = images.shape\n",
        "    cls_tokens = repeat(self.special_token, '() n e -> b n e', b=b)\n",
        "    tokenized_patches = torch.cat([cls_tokens,linear_output], dim=1)\n",
        "\n",
        "\n",
        "\n",
        "    ## Add positional embeddings\n",
        "    position_embeddings = get_pos_embeddings(self.d,tokenized_patches)\n",
        "\n",
        "    input_embeddings = tokenized_patches+position_embeddings\n",
        "\n",
        "\n",
        "\n",
        "    ## Pass through encoder\n",
        "\n",
        "\n",
        "    ecoder_output = self.encoder(input_embeddings)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Get classification token\n",
        "\n",
        "    output = ecoder_output[:,0:1,:]\n",
        "    output_sequeesed = output.squeeze()\n",
        "\n",
        "\n",
        "\n",
        "    ## Pass through classifier\n",
        "    classifier_output = self.classifier(output_sequeesed)\n",
        "\n",
        "\n",
        "    return classifier_output\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_patches(input_image,patch_size):\n",
        "  patches = rearrange(input_image, 'b c (h p1) (w p2)  -> b (p1 p2) (h w c)', p1=patch_size, p2=patch_size)    # Create patch of the input image\n",
        "  return patches"
      ],
      "metadata": {
        "id": "kSIoKOk8_ee7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class linearmap(nn.Module):\n",
        "  def __init__(self,image_dim,n_patches,emb_size):\n",
        "    super().__init__()\n",
        "    self.projection = nn.Sequential(nn.Linear((image_dim[2]//n_patches)**2,emb_size))\n",
        "\n",
        "  def forward(self,x : Tensor) -> Tensor:\n",
        "\n",
        "    x = self.projection(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "dOy4ukg3NkP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pos_embeddings(emb_size, patches_with_clstokens):\n",
        "    d_model=emb_size\n",
        "    length=patches_with_clstokens.shape[1]\n",
        "    b=patches_with_clstokens.shape[0]\n",
        "    if d_model % 2 != 0:\n",
        "        raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
        "                         \"odd dim (got dim={:d})\".format(d_model))\n",
        "\n",
        "    pe = torch.zeros(length, d_model)\n",
        "    position = torch.arange(0, length).unsqueeze(1)\n",
        "    div_term = torch.exp((torch.arange(0, d_model, 2, dtype=torch.float) *\n",
        "                         -(math.log(10000.0) / d_model)))\n",
        "    pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
        "    pe = pe.unsqueeze(0)\n",
        "\n",
        "    pe = repeat(pe, '() n e -> b n e', b=b)\n",
        "    return pe"
      ],
      "metadata": {
        "id": "rGuQtJ4X_9Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4-PNLU5rw_u"
      },
      "outputs": [],
      "source": [
        "class MHSA(nn.Module):\n",
        "    def __init__(self,d=8, n_heads=2): # d: dimension of embedding spacr, n_head: dimension of attention heads\n",
        "        super(MHSA, self).__init__()\n",
        "\n",
        "        self.d = d\n",
        "        self.n_heads= n_heads\n",
        "\n",
        "        self.keys = nn.Linear(d, d)\n",
        "        self.queries = nn.Linear(d, d)\n",
        "        self.values = nn.Linear(d, d)\n",
        "\n",
        "\n",
        "        self.projection = nn.Linear(d, d)\n",
        "\n",
        "    def forward(self,x: Tensor) -> Tensor:\n",
        "\n",
        "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.n_heads)\n",
        "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.n_heads)\n",
        "        values  = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.n_heads)\n",
        "\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
        "\n",
        "\n",
        "        scaling = self.d ** (1/2)\n",
        "        attention_score = F.softmax(energy, dim=-1) / scaling\n",
        "\n",
        "        # sum up over the third axis\n",
        "        out = torch.einsum('bhal, bhlv -> bhav ', attention_score, values)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        out = self.projection(out)\n",
        "        return out\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wr5lLo2rw_2"
      },
      "outputs": [],
      "source": [
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, hidden_d, n_heads):\n",
        "        super(ViTEncoder, self).__init__()\n",
        "        self.hidden_d = hidden_d\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(hidden_d) # Add Layer-Norm\n",
        "        self.mhsa = MHSA(hidden_d, n_heads)\n",
        "        self.norm2 = nn.LayerNorm(hidden_d) # Add another Layer-Norm\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_d, 4 * hidden_d),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * hidden_d, hidden_d)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.mhsa.forward(self.norm1(x)) + x     # x is input embedding i.e. output from LightViT.forward\n",
        "        out = out + self.mlp(self.norm2(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "klFcSImirw_5"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "  model = ViTEncoder(hidden_d=8, n_heads=2)\n",
        "\n",
        "  x = torch.randn(7, 50, 8)\n",
        "  print(model(x).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga5LpLynrw_9"
      },
      "outputs": [],
      "source": [
        "def load_mnist_dataset():\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "\n",
        "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(train_dataset,batch_size = 64)\n",
        "\n",
        "\n",
        "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    test_data_loader = torch.utils.data.DataLoader(test_dataset,batch_size = 64)\n",
        "\n",
        "    return train_data_loader, test_data_loader\n",
        "\n",
        "train_dataset, test_dataset = load_mnist_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for img,lbl in train_dataset:\n",
        "  image_dim = img.shape\n",
        "  break"
      ],
      "metadata": {
        "id": "ATJBoOszQcWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define Model\n",
        "\n",
        "my_model = LightViT(image_dim)\n",
        "\n",
        "\n",
        "# Define Optimizer\n",
        "\n",
        "optimizer_used = torch.optim.Adam(my_model.parameters(), lr=0.005)\n",
        "\n",
        "# Define Loss\n",
        "\n",
        "loss_criteria = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "zHVPCh6LFWyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = []\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "def model_training(epoches,model,criteria,optimizer,train_data,test_data):\n",
        "\n",
        "  for epoch in range(epoches):\n",
        "\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for image, labels in train_data:\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      layer_output = model.forward(image)\n",
        "\n",
        "\n",
        "      loss = criteria(layer_output,labels)\n",
        "\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      _,predicted = torch.max(layer_output.data,1)\n",
        "\n",
        "\n",
        "      train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "\n",
        "      train_total += labels.size(0)\n",
        "\n",
        "\n",
        "    epoch_train_accuracy = train_correct/train_total\n",
        "    epoch_train_loss = (train_loss/len(train_data))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    test_loss = 0\n",
        "    test_correct = 0\n",
        "    test_total=0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      for test_images, test_labels in test_data:\n",
        "\n",
        "\n",
        "        test_layer_outputs = model.forward(test_images)\n",
        "\n",
        "        loss = criteria(test_layer_outputs,test_labels)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        _,predicted = torch.max(test_layer_outputs.data,1)\n",
        "\n",
        "        test_total  += test_labels.size(0)\n",
        "\n",
        "\n",
        "        test_correct += (predicted == test_labels).sum().item()\n",
        "\n",
        "    epoch_test_accuracy = test_correct/test_total\n",
        "    epoch_test_loss =test_loss / len(test_data)\n",
        "\n",
        "    print('\\nEpoch = ', epoch , 'Training Accuracy = ',epoch_train_accuracy, 'Training loss = ',epoch_train_loss )\n",
        "    print('\\nEpoch = ', epoch , 'Testing Accuracy = ',epoch_test_accuracy, 'Testing loss = ',epoch_test_loss )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    test_accuracies.append(epoch_test_accuracy)\n",
        "\n",
        "    test_losses.append(epoch_test_loss)\n",
        "\n",
        "    train_losses.append(epoch_train_loss)\n",
        "\n",
        "    train_accuracies.append(epoch_train_accuracy)\n",
        "\n",
        "    epochs.append(epoch)\n",
        "\n",
        "\n",
        "  return train_accuracies,train_losses,test_accuracies,test_losses,epochs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wsV967yOFk-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_accuracies, train_losses, test_accuracy, test_losses,epoch = model_training(\n",
        "#     epoches=10, model=my_model, criteria=loss_criteria, optimizer=optimizer_used, train_data=train_dataset,\n",
        "    test_data=test_dataset)\n"
      ],
      "metadata": {
        "id": "cEVQ78cBqDuw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epoch, train_accuracies, marker='o', label='Train Accuracy')\n",
        "plt.plot(epoch, test_accuracy, marker='o', label='Test Accuracy')\n",
        "plt.title('Train Accuracy and Test Accuracy vs. Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "soq68bhxcthG"
      },
      "execution_count": 3,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}